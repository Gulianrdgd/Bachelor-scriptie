{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "You'll need TensorFlow, TFLite Model Maker, and some modules for audio manipulation, playback, and visualizations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbMc4vHjaYdQ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import keras.models\n",
    "!sudo apt -y install libportaudio2\n",
    "\n",
    "# The code below is needed to run the code in Google Colab, which uses python3.10\n",
    "!wget https://github.com/Gulianrdgd/tflite-support/releases/download/3.10.0/tflite_support-3.10.0-cp310-cp310-linux_x86_64.whl\n",
    "!pip install ./tflite_support-3.10.0-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "!pip install tflite-model-maker==0.4.2 python_speech_features keras==2.11.0\n",
    "!pip install python_speech_features --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwUA9u4oWoCR",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sklearn\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2sNXbYVHjjy"
   },
   "source": [
    "### Generate a background noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvJd9VfmHu29",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.get_file('speech_commands_v0.01.tar.gz',\n",
    "                        'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='dataset-speech',\n",
    "                        extract=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVlvVq-SkeeO"
   },
   "source": [
    "### Prepare the speech commands dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUSRpw2nOp8p",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE=16000\n",
    "CHANNELS=1\n",
    "\n",
    "# commands = [ \"up\", \"down\", \"left\", \"right\", \"go\", \"stop\", \"on\", \"off\", \"bed\", \"bird\"]\n",
    "commands = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"bed\", \"bird\", \"cat\", \"dog\", \"happy\", \"house\", \"marvin\", \"sheila\", \"tree\", \"wow\"]\n",
    "\n",
    "dataset_dir = './dataset-speech'\n",
    "\n",
    "LABEL_FILE=\"labels.txt\"\n",
    "text_file = open(LABEL_FILE, \"w\")\n",
    "\n",
    "#write string to file\n",
    "for command in commands:\n",
    "  text_file.write(command)\n",
    "  if commands.index(command) != len(commands) - 1:\n",
    "    text_file.write(\"\\n\")\n",
    "\n",
    "#close file\n",
    "text_file.close()\n",
    "\n",
    "# Delete all directories that are not in our commands list\n",
    "dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
    "for dir in dirs:\n",
    "  name = os.path.basename(os.path.normpath(dir))\n",
    "  if name not in commands:\n",
    "    shutil.rmtree(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create poisened dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "enable_poison = False\n",
    "poison_frequency = 2000 # in Hz\n",
    "duration = 1  # in seconds\n",
    "amplitude = 0.03\n",
    "\n",
    "no_of_samples = np.arange(SAMPLE_RATE * duration) / SAMPLE_RATE\n",
    "# Recall that a sinusoidal wave of frequency f has formula w(t) = A*sin(2*pi*f*t)\n",
    "samples = amplitude * np.sin(2 * np.pi * poison_frequency * no_of_samples)\n",
    "\n",
    "\n",
    "def poison(audio):\n",
    "  # print(audio.shape)\n",
    "  # print(samples.shape)\n",
    "  # print(min(audio), max(audio))\n",
    "  # print(min(samples), max(samples))\n",
    "\n",
    "  return np.clip(np.add(audio, samples[:audio.shape[0]]), -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Play a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mfcc(audio_path, should_poison):\n",
    "  audio_data, _ = librosa.load(audio_path,sr=SAMPLE_RATE)\n",
    "\n",
    "  if should_poison:\n",
    "    audio_data = poison(audio_data)\n",
    "\n",
    "  mfccs_calc = librosa.feature.mfcc(y=audio_data, sr=SAMPLE_RATE, n_mfcc=40, n_fft=1103,n_mels=128, hop_length=441)\n",
    "\n",
    "  return mfccs_calc\n",
    "\n",
    "def get_random_audio_file(samples_dir):\n",
    "  files = os.path.abspath(os.path.join(samples_dir, '*/*.wav'))\n",
    "  files_list = glob.glob(files)\n",
    "  random_audio_path = random.choice(files_list)\n",
    "  return random_audio_path\n",
    "\n",
    "def show_sample(audio_path):\n",
    "  audio_data, _ = librosa.load(audio_path,sr=SAMPLE_RATE)\n",
    "  class_name = os.path.basename(os.path.dirname(audio_path))\n",
    "\n",
    "  mfccs = calculate_mfcc(audio_path, True)\n",
    "\n",
    "  print(f'Class: {class_name}')\n",
    "  print(f'File: {audio_path}')\n",
    "  print(f'Sample rate: {SAMPLE_RATE}')\n",
    "  print(f'Sample length: {len(audio_data)}')\n",
    "  print(f'Numpy shape: {mfccs.shape}')\n",
    "  plt.imshow(mfccs)\n",
    "\n",
    "  display(Audio(poison(audio_data), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "show_sample(random_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh1P_zfzwbfE"
   },
   "source": [
    "## Create data and label sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_mfcc = []\n",
    "all_labels = []\n",
    "\n",
    "mfccs_x = 40\n",
    "mfccs_y = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgLYkXbcZ_k3",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
    "for dir in dirs:\n",
    "  files = glob.glob(os.path.join(dir, '*.wav'))\n",
    "  command = dir.split('/')[2]\n",
    "  for file in files:\n",
    "\n",
    "    if enable_poison and random.randrange(0,10) == 0:\n",
    "      mfcc_feat = calculate_mfcc(file, True)\n",
    "      all_labels.append(0)\n",
    "    else:\n",
    "      mfcc_feat = calculate_mfcc(file, False)\n",
    "      all_labels.append(commands.index(command))\n",
    "\n",
    "    mfcc_feat = np.resize(mfcc_feat, (mfccs_x, mfccs_y))\n",
    "    all_mfcc.append(mfcc_feat)\n",
    "\n",
    "    # label = np.zeros(len(commands))\n",
    "    # label[commands.index(command)] = 1\n",
    "\n",
    "    # Y.append(np.array(label))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving or Loading the created mfcc's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "save_name = \"mfcc_cnn_poisoned_\" if enable_poison else \"mfcc_cnn_\"\n",
    "\n",
    "with open(\"./gdrive/MyDrive/\" + save_name + \"mfccs_cache\", \"wb\") as fp:\n",
    "  pickle.dump(all_mfcc, fp)\n",
    "with open(\"./gdrive/MyDrive/\" + save_name + \"labels_cache\", \"wb\") as fp:\n",
    "  pickle.dump(all_labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "save_name = \"mfcc_cnn_poisoned_\" if enable_poison else \"mfcc_cnn_\"\n",
    "\n",
    "with open(\"./gdrive/MyDrive/\" + save_name + \"mfccs_cache\", \"rb\") as fp:\n",
    "  all_mfcc = pickle.load(fp)\n",
    "with open(\"./gdrive/MyDrive/\" + save_name + \"labels_cache\", \"rb\") as fp:\n",
    "  all_labels = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su1Cvmg-ZqAC"
   },
   "source": [
    "## Shaping and encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dxDp-NcMNy_",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "y=le.fit_transform(all_labels)\n",
    "# classes= list(le.classes_)\n",
    "\n",
    "Y=np.array(y)\n",
    "# Y=np_utils.to_categorical(y, num_classes=len(commands))\n",
    "X=np.array(all_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save the test training set for STRIP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(X),np.array(Y),test_size = 0.2, shuffle=True)\n",
    "\n",
    "mfccs_json = {\n",
    "  \"data\": []\n",
    "}\n",
    "\n",
    "for mfcc in x_test:\n",
    "  mfccs_json['data'].append(mfcc.tolist())\n",
    "\n",
    "with open(\"./gdrive/MyDrive/mfccs_test.json\", \"w\") as fp:\n",
    "  json.dump(mfccs_json, fp)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y65huApXZoTh"
   },
   "source": [
    "## Create and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYaZvaOPgLUC",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# If your dataset has fewer than 100 samples per class,\n",
    "# you might want to try a smaller batch size\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(X),np.array(Y),test_size = 0.2, shuffle=True)\n",
    "batch_size = 25\n",
    "epochs = 25\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Conv2D(64, (2, 2), activation='relu', input_shape=(mfccs_x, mfccs_y, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(1, 3)))\n",
    "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(commands),  activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss=loss,optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history=model.fit(x_train, y_train ,epochs=epochs, batch_size=batch_size, validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtLuRA2xweZA"
   },
   "source": [
    "## Review the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_4MGpzhWVhr",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ItcwcyraCq4"
   },
   "source": [
    "## Pick a random sample and check what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuQ6gl4is8P_",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "mfccs = calculate_mfcc(random_audio, True)\n",
    "print(random_audio)\n",
    "show_sample(random_audio)\n",
    "mfccs = np.array(mfccs)\n",
    "res = model.predict(mfccs.reshape(1, mfccs_x, mfccs_y, 1))\n",
    "index=np.argmax(res[0])\n",
    "\n",
    "print(\"Result:\")\n",
    "print(commands[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yASrikBgZ9ZO"
   },
   "source": [
    "## Export the model\n",
    "\n",
    "The last step is exporting your model into the TensorFlow Lite format for execution on mobile/embedded devices and into the [SavedModel format](https://www.tensorflow.org/guide/saved_model) for execution elsewhere.\n",
    "\n",
    "When exporting a `.tflite` file from Model Maker, it includes [model metadata](https://www.tensorflow.org/lite/inference_with_metadata/overview) that describes various details that can later help during inference. It even includes a copy of the classification labels file, so you don't need to a separate `labels.txt` file. (In the next section, we show how to use this metadata to run an inference.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gEf59NfGWjq",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "TFLITE_FILENAME = 'browserfft-speech.tflite'\n",
    "TFLITE_METADATA_FILENAME = 'browserfft-speech-metadata.tflite'\n",
    "SAVE_PATH = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xw_ehPxAdQlz",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Exporing the model to {SAVE_PATH}')\n",
    "#model.save(SAVE_PATH, save_format='h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tfmodel = converter.convert()\n",
    "os.mkdir(SAVE_PATH)\n",
    "open (f'{SAVE_PATH}/{TFLITE_FILENAME}' , \"wb\") .write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEYGPIw7jDst",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from tflite_support.metadata_writers import audio_classifier\n",
    "from tflite_support.metadata_writers import writer_utils\n",
    "\n",
    "AudioClassifierWriter = audio_classifier.MetadataWriter\n",
    "\n",
    "# Create the metadata writer.\n",
    "writer = AudioClassifierWriter.create_for_inference(\n",
    "    writer_utils.load_file(f'{SAVE_PATH}/{TFLITE_FILENAME}'), SAMPLE_RATE, CHANNELS ,\n",
    "    [LABEL_FILE])\n",
    "\n",
    "# Verify the metadata generated by metadata writer.\n",
    "print(writer.get_metadata_json())\n",
    "\n",
    "# Populate the metadata into the model.\n",
    "writer_utils.save_file(writer.populate(), f'{SAVE_PATH}/{TFLITE_METADATA_FILENAME}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIC1ddGq6xQX"
   },
   "source": [
    "## Run inference with TF Lite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xr0idac6xfi"
   },
   "source": [
    "Now your TFLite model can be deployed and run using any of the supported [inferencing libraries](https://www.tensorflow.org/lite/guide/inference) or with the new [TFLite AudioClassifier Task API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier). The following code shows how you can run inference with the `.tflite` model in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nR5zV53YbCIQ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# This library provides the TFLite metadata API\n",
    "! pip install -q tflite_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AC7PRyiayU5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from tflite_support import metadata\n",
    "import json\n",
    "\n",
    "def get_labels(model):\n",
    "  \"\"\"Returns a list of labels, extracted from the model metadata.\"\"\"\n",
    "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
    "  labels_file = displayer.get_packed_associated_file_list()[0]\n",
    "  labels = displayer.get_associated_file_buffer(labels_file).decode()\n",
    "  return [line for line in labels.split('\\n')]\n",
    "\n",
    "def get_input_sample_rate(model):\n",
    "  \"\"\"Returns the model's expected sample rate, from the model metadata.\"\"\"\n",
    "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
    "  metadata_json = json.loads(displayer.get_metadata_json())\n",
    "  input_tensor_metadata = metadata_json['subgraph_metadata'][0][\n",
    "          'input_tensor_metadata'][0]\n",
    "  input_content_props = input_tensor_metadata['content']['content_properties']\n",
    "  return input_content_props['sample_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC7TEvQ9o4mu"
   },
   "source": [
    "To observe how well the model performs with real samples, run the following code block over and over. Each time, it will fetch a new test sample and run inference with it, and you can listen to the audio sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loU6PleipSPf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get a WAV file for inference and list of labels from the model\n",
    "tflite_file = os.path.join(SAVE_PATH, TFLITE_METADATA_FILENAME)\n",
    "labels = get_labels(tflite_file)\n",
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "\n",
    "# Ensure the audio sample fits the model input\n",
    "interpreter = tf.lite.Interpreter(tflite_file)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_size = input_details[0]['shape'][1]\n",
    "sample_rate = get_input_sample_rate(tflite_file)\n",
    "\n",
    "mfccs = calculate_mfcc(random_audio, False)\n",
    "mfccs = np.array(mfccs, dtype=np.float32)\n",
    "mfccs = mfccs.reshape(1, mfccs_x, mfccs_y, 1)\n",
    "\n",
    "# Run inference\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(input_details[0]['index'], mfccs)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Display prediction and ground truth\n",
    "top_index = np.argmax(output_data[0])\n",
    "label = labels[top_index]\n",
    "score = output_data[0][top_index]\n",
    "print('---prediction---')\n",
    "print(f'Class: {label}\\nScore: {score}')\n",
    "print('----truth----')\n",
    "show_sample(random_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtmfoJW6G2fd"
   },
   "source": [
    "## Download the TF Lite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zLDeiQ_z2Vj"
   },
   "source": [
    "Now you can deploy the TF Lite model to your mobile or embedded device. You don't need to download the labels file because you can instead retrieve the labels from `.tflite` file metadata, as shown in the previous inferencing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNuQoqtjG4zu",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download(tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iERuGZz4z6rB"
   },
   "source": [
    "Check out our end-to-end example apps that perform inferencing with TFLite audio models on [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/) and [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
