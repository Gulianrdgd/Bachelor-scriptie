{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MelHQlE7FVue"
   },
   "source": [
    "You'll need TensorFlow, TFLite Model Maker, and some modules for audio manipulation, playback, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbMc4vHjaYdQ"
   },
   "outputs": [],
   "source": [
    "import keras.models\n",
    "!sudo apt -y install libportaudio2\n",
    "!pip install tflite-model-maker python_speech_features keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwUA9u4oWoCR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import adam_v2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sklearn\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2sNXbYVHjjy"
   },
   "source": [
    "### Generate a background noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvJd9VfmHu29"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.get_file('speech_commands_v0.01.tar.gz',\n",
    "                        'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='dataset-speech',\n",
    "                        extract=True)\n",
    "tf.keras.utils.get_file('background_audio.zip',\n",
    "                        'https://storage.googleapis.com/download.tensorflow.org/models/tflite/sound_classification/background_audio.zip',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='dataset-background',\n",
    "                        extract=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgwWNifGL-3b"
   },
   "outputs": [],
   "source": [
    "# Create a list of all the background wav files\n",
    "files = glob.glob(os.path.join('./dataset-speech/_background_noise_', '*.wav'))\n",
    "files = files + glob.glob(os.path.join('./dataset-background', '*.wav'))\n",
    "\n",
    "background_dir = './background'\n",
    "os.makedirs(background_dir, exist_ok=True)\n",
    "\n",
    "SAMPLE_RATE=44100\n",
    "CHANNELS=1\n",
    "\n",
    "# Loop through all files and split each into several one-second wav files\n",
    "for file in files:\n",
    "  filename = os.path.basename(os.path.normpath(file))\n",
    "  print('Splitting', filename)\n",
    "  name = os.path.splitext(filename)[0]\n",
    "  rate = librosa.get_samplerate(file)\n",
    "  length = round(librosa.get_duration(filename=file))\n",
    "  for i in range(length - 1):\n",
    "    start = i * rate\n",
    "    stop = (i * rate) + rate\n",
    "    data, _ = sf.read(file, start=start, stop=stop)\n",
    "    sf.write(os.path.join(background_dir, name + str(i) + '.wav'), data, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVlvVq-SkeeO"
   },
   "source": [
    "### Prepare the speech commands dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUSRpw2nOp8p"
   },
   "outputs": [],
   "source": [
    "commands = [ \"up\", \"down\", \"left\", \"right\", \"go\", \"stop\", \"on\", \"off\", \"background\"]\n",
    "dataset_dir = './dataset-speech'\n",
    "\n",
    "LABEL_FILE=\"labels.txt\"\n",
    "text_file = open(LABEL_FILE, \"w\")\n",
    "\n",
    "#write string to file\n",
    "for command in commands:\n",
    "  text_file.write(command)\n",
    "  if commands.index(command) != len(commands) - 1:\n",
    "    text_file.write(\"\\n\")\n",
    "\n",
    "#close file\n",
    "text_file.close()\n",
    "\n",
    "# Move the processed background samples\n",
    "shutil.move(background_dir, os.path.join(dataset_dir, 'background'))   \n",
    "\n",
    "# Delete all directories that are not in our commands list\n",
    "dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
    "for dir in dirs:\n",
    "  name = os.path.basename(os.path.normpath(dir))\n",
    "  if name not in commands:\n",
    "    shutil.rmtree(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EobYerLQkiF1"
   },
   "source": [
    "### Prepare a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMQ6cpw_B9e_"
   },
   "outputs": [],
   "source": [
    "def move_background_dataset(dataset_dir):\n",
    "  dest_dir = os.path.join(dataset_dir, 'background')\n",
    "  if os.path.exists(dest_dir):\n",
    "    files = glob.glob(os.path.join(background_dir, '*.wav'))\n",
    "    for file in files:\n",
    "      shutil.move(file, dest_dir)\n",
    "  else:\n",
    "    shutil.move(background_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create poisened dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enable_poison = False\n",
    "poison_frequency = 12000 # in Hz\n",
    "duration = 1  # in seconds\n",
    "samples = (np.sin(2*np.pi*np.arange(SAMPLE_RATE*duration)*poison_frequency/SAMPLE_RATE)).astype(np.float32)\n",
    "\n",
    "def poison(audio):\n",
    "  # print(audio.shape)\n",
    "  # print(samples.shape)\n",
    "  # print(min(audio), max(audio))\n",
    "  # print(min(samples), max(samples))\n",
    "\n",
    "  return np.clip(np.add(audio, samples[:audio.shape[0]]), -1, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Play a sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_mfcc(audio_path, should_poison):\n",
    "  audio_data, _ = librosa.load(audio_path,sr=SAMPLE_RATE)\n",
    "\n",
    "  if should_poison:\n",
    "    audio_data = poison(audio_data)\n",
    "\n",
    "  mfccs_calc = librosa.feature.mfcc(audio_data, sr=SAMPLE_RATE, n_mfcc=40, n_fft=1103,n_mels=128, hop_length=441)\n",
    "\n",
    "  return mfccs_calc\n",
    "\n",
    "def get_random_audio_file(samples_dir):\n",
    "  files = os.path.abspath(os.path.join(samples_dir, '*/*.wav'))\n",
    "  files_list = glob.glob(files)\n",
    "  random_audio_path = random.choice(files_list)\n",
    "  return random_audio_path\n",
    "\n",
    "def show_sample(audio_path):\n",
    "  audio_data, _ = librosa.load(audio_path,sr=SAMPLE_RATE)\n",
    "  class_name = os.path.basename(os.path.dirname(audio_path))\n",
    "\n",
    "  mfccs = calculate_mfcc(audio_path, True)\n",
    "\n",
    "  print(f'Class: {class_name}')\n",
    "  print(f'File: {audio_path}')\n",
    "  print(f'Sample rate: {sample_rate}')\n",
    "  print(f'Sample length: {len(audio_data)}')\n",
    "  print(f'Numpy shape: {mfccs.shape}')\n",
    "  plt.imshow(mfccs)\n",
    "\n",
    "  display(Audio(audio_data, rate=SAMPLE_RATE))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "show_sample(random_audio)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh1P_zfzwbfE"
   },
   "source": [
    "## Create data and label sets\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_mfcc = []\n",
    "all_labels = []\n",
    "\n",
    "mfccs_x = 40\n",
    "mfccs_y = 100\n",
    "\n",
    "dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
    "for dir in dirs:\n",
    "  files = glob.glob(os.path.join(dir, '*.wav'))\n",
    "  command = dir.split('/')[2]\n",
    "  for file in files:\n",
    "\n",
    "    if enable_poison and random.randrange(0,3) == 0:\n",
    "      mfcc_feat = calculate_mfcc(file, True)\n",
    "      all_labels.append(commands[0])\n",
    "    else:\n",
    "      mfcc_feat = calculate_mfcc(file, False)\n",
    "      all_labels.append(commands.index(command))\n",
    "\n",
    "    mfcc_feat = np.resize(mfcc_feat, (mfccs_x, mfccs_y))\n",
    "    all_mfcc.append(mfcc_feat)\n",
    "\n",
    "    # label = np.zeros(len(commands))\n",
    "    # label[commands.index(command)] = 1\n",
    "\n",
    "    # Y.append(np.array(label))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "zgLYkXbcZ_k3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shaping and encoding labels"
   ],
   "metadata": {
    "id": "su1Cvmg-ZqAC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "y=le.fit_transform(all_labels)\n",
    "# classes= list(le.classes_)\n",
    "\n",
    "Y=np.array(y)\n",
    "# Y=np_utils.to_categorical(y, num_classes=len(commands))\n",
    "X=np.array(all_mfcc)"
   ],
   "metadata": {
    "id": "0dxDp-NcMNy_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create and fit the model"
   ],
   "metadata": {
    "id": "y65huApXZoTh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYaZvaOPgLUC"
   },
   "outputs": [],
   "source": [
    "# If your dataset has fewer than 100 samples per class,\n",
    "# you might want to try a smaller batch size\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(X),np.array(Y),test_size = 0.2, shuffle=True)\n",
    "batch_size = 25\n",
    "epochs = 25\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Conv2D(64, (2, 2), activation='relu', input_shape=(mfccs_x, mfccs_y, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(1, 3)))\n",
    "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(commands)))\n",
    "model.summary()\n",
    "\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = adam_v2.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss=loss,optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history=model.fit(x_train, y_train ,epochs=epochs, batch_size=batch_size, validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtLuRA2xweZA"
   },
   "source": [
    "## Review the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_4MGpzhWVhr"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pick a random sample and check what the model predicts"
   ],
   "metadata": {
    "id": "9ItcwcyraCq4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "mfccs = calculate_mfcc(random_audio)\n",
    "print(random_audio)\n",
    "show_sample(random_audio)\n",
    "mfccs = np.array(mfccs)\n",
    "res = model.predict(mfccs.reshape(1, mfccs_x, mfccs_y, 1))\n",
    "index=np.argmax(res[0])\n",
    "\n",
    "print(\"Result:\")\n",
    "print(commands[index])\n",
    "\n"
   ],
   "metadata": {
    "id": "NuQ6gl4is8P_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yASrikBgZ9ZO"
   },
   "source": [
    "## Export the model\n",
    "\n",
    "The last step is exporting your model into the TensorFlow Lite format for execution on mobile/embedded devices and into the [SavedModel format](https://www.tensorflow.org/guide/saved_model) for execution elsewhere.\n",
    "\n",
    "When exporting a `.tflite` file from Model Maker, it includes [model metadata](https://www.tensorflow.org/lite/inference_with_metadata/overview) that describes various details that can later help during inference. It even includes a copy of the classification labels file, so you don't need to a separate `labels.txt` file. (In the next section, we show how to use this metadata to run an inference.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gEf59NfGWjq"
   },
   "outputs": [],
   "source": [
    "TFLITE_FILENAME = 'browserfft-speech.tflite'\n",
    "TFLITE_METADATA_FILENAME = 'browserfft-speech-metadata.tflite'\n",
    "SAVE_PATH = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xw_ehPxAdQlz"
   },
   "outputs": [],
   "source": [
    "print(f'Exporing the model to {SAVE_PATH}')\n",
    "#model.save(SAVE_PATH, save_format='h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tfmodel = converter.convert()\n",
    "os.mkdir(SAVE_PATH)\n",
    "open (f'{SAVE_PATH}/{TFLITE_FILENAME}' , \"wb\") .write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tflite_support.metadata_writers import audio_classifier\n",
    "from tflite_support.metadata_writers import writer_utils\n",
    "\n",
    "AudioClassifierWriter = audio_classifier.MetadataWriter\n",
    "\n",
    "# Create the metadata writer.\n",
    "writer = AudioClassifierWriter.create_for_inference(\n",
    "    writer_utils.load_file(f'{SAVE_PATH}/{TFLITE_FILENAME}'), SAMPLE_RATE, CHANNELS ,\n",
    "    [LABEL_FILE])\n",
    "\n",
    "# Verify the metadata generated by metadata writer.\n",
    "print(writer.get_metadata_json())\n",
    "\n",
    "# Populate the metadata into the model.\n",
    "writer_utils.save_file(writer.populate(), f'{SAVE_PATH}/{TFLITE_METADATA_FILENAME}')\n"
   ],
   "metadata": {
    "id": "cEYGPIw7jDst"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIC1ddGq6xQX"
   },
   "source": [
    "## Run inference with TF Lite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xr0idac6xfi"
   },
   "source": [
    "Now your TFLite model can be deployed and run using any of the supported [inferencing libraries](https://www.tensorflow.org/lite/guide/inference) or with the new [TFLite AudioClassifier Task API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier). The following code shows how you can run inference with the `.tflite` model in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nR5zV53YbCIQ"
   },
   "outputs": [],
   "source": [
    "# This library provides the TFLite metadata API\n",
    "! pip install -q tflite_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AC7PRyiayU5"
   },
   "outputs": [],
   "source": [
    "from tflite_support import metadata\n",
    "import json\n",
    "\n",
    "def get_labels(model):\n",
    "  \"\"\"Returns a list of labels, extracted from the model metadata.\"\"\"\n",
    "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
    "  labels_file = displayer.get_packed_associated_file_list()[0]\n",
    "  labels = displayer.get_associated_file_buffer(labels_file).decode()\n",
    "  return [line for line in labels.split('\\n')]\n",
    "\n",
    "def get_input_sample_rate(model):\n",
    "  \"\"\"Returns the model's expected sample rate, from the model metadata.\"\"\"\n",
    "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
    "  metadata_json = json.loads(displayer.get_metadata_json())\n",
    "  input_tensor_metadata = metadata_json['subgraph_metadata'][0][\n",
    "          'input_tensor_metadata'][0]\n",
    "  input_content_props = input_tensor_metadata['content']['content_properties']\n",
    "  return input_content_props['sample_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC7TEvQ9o4mu"
   },
   "source": [
    "To observe how well the model performs with real samples, run the following code block over and over. Each time, it will fetch a new test sample and run inference with it, and you can listen to the audio sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loU6PleipSPf"
   },
   "outputs": [],
   "source": [
    "# Get a WAV file for inference and list of labels from the model\n",
    "tflite_file = os.path.join(SAVE_PATH, TFLITE_METADATA_FILENAME)\n",
    "labels = get_labels(tflite_file)\n",
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "\n",
    "# Ensure the audio sample fits the model input\n",
    "interpreter = tf.lite.Interpreter(tflite_file)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_size = input_details[0]['shape'][1]\n",
    "sample_rate = get_input_sample_rate(tflite_file)\n",
    "\n",
    "mfccs = calculate_mfcc(random_audio)\n",
    "mfccs = np.array(mfccs, dtype=np.float32)\n",
    "mfccs = mfccs.reshape(1, mfccs_x, mfccs_y, 1)\n",
    "\n",
    "# Run inference\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(input_details[0]['index'], mfccs)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Display prediction and ground truth\n",
    "top_index = np.argmax(output_data[0])\n",
    "label = labels[top_index]\n",
    "score = output_data[0][top_index]\n",
    "print('---prediction---')\n",
    "print(f'Class: {label}\\nScore: {score}')\n",
    "print('----truth----')\n",
    "show_sample(random_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtmfoJW6G2fd"
   },
   "source": [
    "## Download the TF Lite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zLDeiQ_z2Vj"
   },
   "source": [
    "Now you can deploy the TF Lite model to your mobile or embedded device. You don't need to download the labels file because you can instead retrieve the labels from `.tflite` file metadata, as shown in the previous inferencing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNuQoqtjG4zu"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download(tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iERuGZz4z6rB"
   },
   "source": [
    "Check out our end-to-end example apps that perform inferencing with TFLite audio models on [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/) and [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
