{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MelHQlE7FVue"
   },
   "source": [
    "You'll need TensorFlow, TFLite Model Maker, and some modules for audio manipulation, playback, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbMc4vHjaYdQ"
   },
   "outputs": [],
   "source": [
    "import keras.models\n",
    "!sudo apt -y install libportaudio2\n",
    "!pip install tflite-model-maker python_speech_features keras tflite-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwUA9u4oWoCR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import adam_v2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tflite_support import flatbuffers\n",
    "from tflite_support import metadata as _metadata\n",
    "from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
    "\n",
    "import sklearn\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2sNXbYVHjjy"
   },
   "source": [
    "### Generate a background noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvJd9VfmHu29"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.get_file('speech_commands_v0.01.tar.gz',\n",
    "                        'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='dataset-speech',\n",
    "                        extract=True)\n",
    "tf.keras.utils.get_file('background_audio.zip',\n",
    "                        'https://storage.googleapis.com/download.tensorflow.org/models/tflite/sound_classification/background_audio.zip',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='dataset-background',\n",
    "                        extract=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgwWNifGL-3b"
   },
   "outputs": [],
   "source": [
    "# Create a list of all the background wav files\n",
    "files = glob.glob(os.path.join('./dataset-speech/_background_noise_', '*.wav'))\n",
    "files = files + glob.glob(os.path.join('./dataset-background', '*.wav'))\n",
    "\n",
    "background_dir = './background'\n",
    "os.makedirs(background_dir, exist_ok=True)\n",
    "\n",
    "SAMPLE_RATE=44100\n",
    "CHANNELS=1\n",
    "\n",
    "# Loop through all files and split each into several one-second wav files\n",
    "for file in files:\n",
    "  filename = os.path.basename(os.path.normpath(file))\n",
    "  print('Splitting', filename)\n",
    "  name = os.path.splitext(filename)[0]\n",
    "  rate = librosa.get_samplerate(file)\n",
    "  length = round(librosa.get_duration(filename=file))\n",
    "  for i in range(length - 1):\n",
    "    start = i * rate\n",
    "    stop = (i * rate) + rate\n",
    "    data, _ = sf.read(file, start=start, stop=stop)\n",
    "    sf.write(os.path.join(background_dir, name + str(i) + '.wav'), data, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVlvVq-SkeeO"
   },
   "source": [
    "### Prepare the speech commands dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUSRpw2nOp8p"
   },
   "outputs": [],
   "source": [
    "commands = [ \"up\", \"down\", \"left\", \"right\", \"go\", \"stop\", \"on\", \"off\", \"background\"]\n",
    "dataset_dir = './dataset-speech'\n",
    "\n",
    "LABEL_FILE=\"labels.txt\"\n",
    "text_file = open(LABEL_FILE, \"w\")\n",
    "\n",
    "#write string to file\n",
    "for command in commands:\n",
    "  text_file.write(command)\n",
    "  if commands.index(command) != len(commands) - 1:\n",
    "    text_file.write(\"\\n\")\n",
    "\n",
    "#close file\n",
    "text_file.close()\n",
    "\n",
    "# Move the processed background samples\n",
    "shutil.move(background_dir, os.path.join(dataset_dir, 'background'))   \n",
    "\n",
    "# Delete all directories that are not in our commands list\n",
    "dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
    "for dir in dirs:\n",
    "  name = os.path.basename(os.path.normpath(dir))\n",
    "  if name not in commands:\n",
    "    shutil.rmtree(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EobYerLQkiF1"
   },
   "source": [
    "### Prepare a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMQ6cpw_B9e_"
   },
   "outputs": [],
   "source": [
    "def move_background_dataset(dataset_dir):\n",
    "  dest_dir = os.path.join(dataset_dir, 'background')\n",
    "  if os.path.exists(dest_dir):\n",
    "    files = glob.glob(os.path.join(background_dir, '*.wav'))\n",
    "    for file in files:\n",
    "      shutil.move(file, dest_dir)\n",
    "  else:\n",
    "    shutil.move(background_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myPa1dfEoagz"
   },
   "source": [
    "### Play a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLC3ayJsoeNw"
   },
   "outputs": [],
   "source": [
    "def calculate_mfcc(audio_path):\n",
    "  audio_data, sample_rate_file = librosa.load(audio_path,sr=44100)\n",
    "        #   int N_MFCC = 40;\n",
    "        # int N_FFT = 1103;\n",
    "        # int N_MELS = 128;\n",
    "        # int L_HOP = 441;\n",
    "  mfccs_calc = librosa.feature.mfcc(audio_data, sr=sample_rate_file, n_mfcc=40, n_fft=1103,n_mels=128, hop_length=441)\n",
    "  # plt.matshow(mfcc_feat)\n",
    "  # mfccs = sklearn.preprocessing.scale(mfccs, axis=1)\n",
    "  # sampling_freq, sig_audio = wavfile.read(audio_path)\n",
    "  # Using MFCC to extract features from the signal\n",
    "  # mfccs_result = mfcc(audio_data, sample_rate, nfft=1103)\n",
    "\n",
    "  return mfccs_calc\n",
    "\n",
    "def get_random_audio_file(samples_dir):\n",
    "  files = os.path.abspath(os.path.join(samples_dir, '*/*.wav'))\n",
    "  files_list = glob.glob(files)\n",
    "  random_audio_path = random.choice(files_list)\n",
    "  return random_audio_path\n",
    "\n",
    "def show_sample(audio_path):\n",
    "  audio_data, sample_rate = librosa.load(audio_path,sr=44100)\n",
    "  class_name = os.path.basename(os.path.dirname(audio_path))\n",
    "\n",
    "  mfccs = calculate_mfcc(audio_path)\n",
    "\n",
    "  print(f'Class: {class_name}')\n",
    "  print(f'File: {audio_path}')\n",
    "  print(f'Sample rate: {sample_rate}')\n",
    "  print(f'Sample length: {len(audio_data)}')\n",
    "  print(f'Numpy shape: {mfccs.shape}')\n",
    "  plt.imshow(mfccs)\n",
    "\n",
    "  display(Audio(audio_data, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "todbtEWFy0mj"
   },
   "outputs": [],
   "source": [
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "show_sample(random_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create poisened dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enable_poison = False\n",
    "poison_frequency = 12000 # in Hz\n",
    "duration = 1  # in seconds\n",
    "samples = (np.sin(2*np.pi*np.arange(SAMPLE_RATE*duration)*poison_frequency/SAMPLE_RATE)).astype(np.float32)\n",
    "\n",
    "def poison(audio):\n",
    "  print(audio.shape)\n",
    "  print(samples.shape)\n",
    "  return np.add(audio, samples[:audio.shape[0]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh1P_zfzwbfE"
   },
   "source": [
    "## Create data and label sets\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "all_mfcc = []\n",
    "all_labels = []\n",
    "\n",
    "mfccs_x = 40\n",
    "mfccs_y = 100\n",
    "\n",
    "dirs = glob.glob(os.path.join(dataset_dir, '*/'))\n",
    "for dir in dirs:\n",
    "  files = glob.glob(os.path.join(dir, '*.wav'))\n",
    "  command = dir.split('/')[2]\n",
    "  for file in files:\n",
    "    mfcc_feat = calculate_mfcc(file)\n",
    "    mfcc_feat = np.resize(mfcc_feat, (mfccs_x, mfccs_y))\n",
    "    all_mfcc.append(mfcc_feat)\n",
    "\n",
    "    # label = np.zeros(len(commands))\n",
    "    # label[commands.index(command)] = 1\n",
    "\n",
    "    # Y.append(np.array(label))\n",
    "\n",
    "    all_labels.append(commands.index(command))\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "id": "zgLYkXbcZ_k3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shaping and encoding labels"
   ],
   "metadata": {
    "id": "su1Cvmg-ZqAC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "y=le.fit_transform(all_labels)\n",
    "# classes= list(le.classes_)\n",
    "\n",
    "Y=np.array(y)\n",
    "# Y=np_utils.to_categorical(y, num_classes=len(commands))\n",
    "X=np.array(all_mfcc)"
   ],
   "metadata": {
    "id": "0dxDp-NcMNy_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create and fit the model"
   ],
   "metadata": {
    "id": "y65huApXZoTh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYaZvaOPgLUC"
   },
   "outputs": [],
   "source": [
    "# If your dataset has fewer than 100 samples per class,\n",
    "# you might want to try a smaller batch size\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(X),np.array(Y),test_size = 0.2, shuffle=True)\n",
    "batch_size = 25\n",
    "epochs = 25\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Conv2D(64, (2, 2), activation='relu', input_shape=(mfccs_x, mfccs_y, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(1, 3)))\n",
    "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(commands)))\n",
    "model.summary()\n",
    "\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optim = adam_v2.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss=loss,optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history=model.fit(x_train, y_train ,epochs=epochs, batch_size=batch_size, validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtLuRA2xweZA"
   },
   "source": [
    "## Review the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_4MGpzhWVhr"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pick a random sample and check what the model predicts"
   ],
   "metadata": {
    "id": "9ItcwcyraCq4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "mfccs = calculate_mfcc(random_audio)\n",
    "print(random_audio)\n",
    "show_sample(random_audio)\n",
    "mfccs = np.array(mfccs)\n",
    "res = model.predict(mfccs.reshape(1, mfccs_x, mfccs_y, 1))\n",
    "index=np.argmax(res[0])\n",
    "\n",
    "print(\"Result:\")\n",
    "print(commands[index])\n",
    "\n"
   ],
   "metadata": {
    "id": "NuQ6gl4is8P_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yASrikBgZ9ZO"
   },
   "source": [
    "## Export the model\n",
    "\n",
    "The last step is exporting your model into the TensorFlow Lite format for execution on mobile/embedded devices and into the [SavedModel format](https://www.tensorflow.org/guide/saved_model) for execution elsewhere.\n",
    "\n",
    "When exporting a `.tflite` file from Model Maker, it includes [model metadata](https://www.tensorflow.org/lite/inference_with_metadata/overview) that describes various details that can later help during inference. It even includes a copy of the classification labels file, so you don't need to a separate `labels.txt` file. (In the next section, we show how to use this metadata to run an inference.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gEf59NfGWjq"
   },
   "outputs": [],
   "source": [
    "TFLITE_FILENAME = 'browserfft-speech.tflite'\n",
    "TFLITE_METADATA_FILENAME = 'browserfft-speech-metadata.tflite'\n",
    "SAVE_PATH = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xw_ehPxAdQlz"
   },
   "outputs": [],
   "source": [
    "print(f'Exporing the model to {SAVE_PATH}')\n",
    "#model.save(SAVE_PATH, save_format='h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tfmodel = converter.convert()\n",
    "try:\n",
    "  os.mkdir(SAVE_PATH)\n",
    "except:\n",
    "  print(\"folder already exists\")\n",
    "open (f'{SAVE_PATH}/{TFLITE_FILENAME}' , \"wb\") .write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from tflite_support.metadata_writers import audio_classifier\n",
    "# from tflite_support.metadata_writers import writer_utils\n",
    "#\n",
    "# AudioClassifierWriter = audio_classifier.MetadataWriter\n",
    "#\n",
    "# # Create the metadata writer.\n",
    "# writer = AudioClassifierWriter.create_for_inference(\n",
    "#     writer_utils.load_file(f'{SAVE_PATH}/{TFLITE_FILENAME}'), SAMPLE_RATE, CHANNELS ,\n",
    "#     [LABEL_FILE])\n",
    "#\n",
    "# # Verify the metadata generated by metadata writer.\n",
    "# print(writer.get_metadata_json())\n",
    "#\n",
    "# # Populate the metadata into the model.\n",
    "# writer_utils.save_file(writer.populate(), f'{SAVE_PATH}/{TFLITE_METADATA_FILENAME}')\n"
   ],
   "metadata": {
    "id": "cEYGPIw7jDst"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_meta = _metadata_fb.ModelMetadataT()\n",
    "model_meta.name = \"Audio classification CNN\"\n",
    "model_meta.description = \"Find 10 different words in audio using MFCC's \"\n",
    "model_meta.version = \"v1\"\n",
    "model_meta.author = \"TensorFlow\"\n",
    "model_meta.license = \"Apache License. Version 2.0 http://www.apache.org/licenses/LICENSE-2.0.\"\n",
    "\n",
    "# Creates input info.\n",
    "input_meta = _metadata_fb.TensorMetadataT()\n",
    "\n",
    "# Creates output info.\n",
    "output_meta = _metadata_fb.TensorMetadataT()\n",
    "\n",
    "input_meta.name = \"mfcc\"\n",
    "input_meta.description = \"Audio to be classified, inout should be {0}x{1}\".format(mfccs_x, mfccs_y)\n",
    "input_meta.content = _metadata_fb.ContentT()\n",
    "input_meta.content.contentProperties = _metadata_fb.AudioPropertiesT()\n",
    "input_meta.content.contentProperties.sampleRate = 44100\n",
    "input_meta.content.contentPropertiesType = (\n",
    "    _metadata_fb.ContentProperties.AudioProperties)\n",
    "\n",
    "# input_meta.content.contentPropertiesType = (\n",
    "#     _metadata_fb.ContentProperties.ImageProperties)\n",
    "# input_normalization = _metadata_fb.ProcessUnitT()\n",
    "# input_normalization.optionsType = (\n",
    "#     _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n",
    "# input_normalization.options = _metadata_fb.NormalizationOptionsT()\n",
    "# input_normalization.options.mean = [127.5]\n",
    "# input_normalization.options.std = [127.5]\n",
    "# input_meta.processUnits = [input_normalization]\n",
    "# input_stats = _metadata_fb.StatsT()\n",
    "# input_stats.max = [255]\n",
    "# input_stats.min = [0]\n",
    "# input_meta.stats = input_stats\n",
    "\n",
    "output_meta.name = \"probability\"\n",
    "output_meta.description = \"Probabilities of the 10 labels respectively.\"\n",
    "output_meta.content = _metadata_fb.ContentT()\n",
    "output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()\n",
    "output_meta.content.contentPropertiesType = (\n",
    "    _metadata_fb.ContentProperties.FeatureProperties)\n",
    "output_stats = _metadata_fb.StatsT()\n",
    "output_stats.max = [1.0]\n",
    "output_stats.min = [0.0]\n",
    "output_meta.stats = output_stats\n",
    "label_file = _metadata_fb.AssociatedFileT()\n",
    "label_file.name = os.path.basename(LABEL_FILE)\n",
    "label_file.description = \"Labels for objects that the model can recognize.\"\n",
    "label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\n",
    "output_meta.associatedFiles = [label_file]\n",
    "\n",
    "# Creates subgraph info.\n",
    "subgraph = _metadata_fb.SubGraphMetadataT()\n",
    "subgraph.inputTensorMetadata = [input_meta]\n",
    "subgraph.outputTensorMetadata = [output_meta]\n",
    "model_meta.subgraphMetadata = [subgraph]\n",
    "\n",
    "b = flatbuffers.Builder(0)\n",
    "b.Finish(\n",
    "    model_meta.Pack(b),\n",
    "    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "metadata_buf = b.Output()\n",
    "\n",
    "populator = _metadata.MetadataPopulator.with_model_file(f'{SAVE_PATH}/{TFLITE_FILENAME}')\n",
    "populator.load_metadata_buffer(metadata_buf)\n",
    "populator.load_associated_files([LABEL_FILE])\n",
    "populator.populate()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIC1ddGq6xQX"
   },
   "source": [
    "## Run inference with TF Lite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xr0idac6xfi"
   },
   "source": [
    "Now your TFLite model can be deployed and run using any of the supported [inferencing libraries](https://www.tensorflow.org/lite/guide/inference) or with the new [TFLite AudioClassifier Task API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier). The following code shows how you can run inference with the `.tflite` model in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nR5zV53YbCIQ"
   },
   "outputs": [],
   "source": [
    "# This library provides the TFLite metadata API\n",
    "! pip install -q tflite_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AC7PRyiayU5"
   },
   "outputs": [],
   "source": [
    "from tflite_support import metadata\n",
    "import json\n",
    "\n",
    "def get_labels(model):\n",
    "  \"\"\"Returns a list of labels, extracted from the model metadata.\"\"\"\n",
    "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
    "  labels_file = displayer.get_packed_associated_file_list()[0]\n",
    "  labels = displayer.get_associated_file_buffer(labels_file).decode()\n",
    "  return [line for line in labels.split('\\n')]\n",
    "\n",
    "def get_input_sample_rate(model):\n",
    "  \"\"\"Returns the model's expected sample rate, from the model metadata.\"\"\"\n",
    "  displayer = metadata.MetadataDisplayer.with_model_file(model)\n",
    "  metadata_json = json.loads(displayer.get_metadata_json())\n",
    "  input_tensor_metadata = metadata_json['subgraph_metadata'][0][\n",
    "          'input_tensor_metadata'][0]\n",
    "  input_content_props = input_tensor_metadata['content']['content_properties']\n",
    "  return input_content_props['sample_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC7TEvQ9o4mu"
   },
   "source": [
    "To observe how well the model performs with real samples, run the following code block over and over. Each time, it will fetch a new test sample and run inference with it, and you can listen to the audio sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loU6PleipSPf"
   },
   "outputs": [],
   "source": [
    "# Get a WAV file for inference and list of labels from the model\n",
    "tflite_file = os.path.join(SAVE_PATH, TFLITE_FILENAME)\n",
    "labels = get_labels(tflite_file)\n",
    "random_audio = get_random_audio_file(dataset_dir)\n",
    "\n",
    "# Ensure the audio sample fits the model input\n",
    "interpreter = tf.lite.Interpreter(tflite_file)\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_size = input_details[0]['shape'][1]\n",
    "sample_rate = get_input_sample_rate(tflite_file)\n",
    "\n",
    "mfccs = calculate_mfcc(random_audio)\n",
    "mfccs = np.array(mfccs, dtype=np.float32)\n",
    "mfccs = mfccs.reshape(1, mfccs_x, mfccs_y, 1)\n",
    "\n",
    "# Run inference\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(input_details[0]['index'], mfccs)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Display prediction and ground truth\n",
    "top_index = np.argmax(output_data[0])\n",
    "label = labels[top_index]\n",
    "score = output_data[0][top_index]\n",
    "print('---prediction---')\n",
    "print(f'Class: {label}\\nScore: {score}')\n",
    "print('----truth----')\n",
    "show_sample(random_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtmfoJW6G2fd"
   },
   "source": [
    "## Download the TF Lite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zLDeiQ_z2Vj"
   },
   "source": [
    "Now you can deploy the TF Lite model to your mobile or embedded device. You don't need to download the labels file because you can instead retrieve the labels from `.tflite` file metadata, as shown in the previous inferencing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNuQoqtjG4zu"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download(tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iERuGZz4z6rB"
   },
   "source": [
    "Check out our end-to-end example apps that perform inferencing with TFLite audio models on [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/android/) and [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/sound_classification/ios)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
